{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zeus-seq2seq_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BNkosi/Zeus/blob/master/zeus_seq2seq_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUa44-ARWsNa",
        "colab_type": "text"
      },
      "source": [
        "**Your Google Drive Authenticate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pD69VymLMNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de90080a-2948-4a49-c2fe-277e9d620db5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qpm_w0sCqOgG",
        "colab": {}
      },
      "source": [
        "path =  \"gdrive/My Drive/EDSA/Zeus\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf1-NmZwEMc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir(path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxH8kPfnW-L5",
        "colab_type": "text"
      },
      "source": [
        "**Google Colab using tensorflow latest version, we change it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS-E7YgHzYOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "606c0614-82c6-4cea-e356-78fd60479998"
      },
      "source": [
        "!pip3 install tensorflow==1.0.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     |████████████████████████████████| 44.5MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (49.2.0)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed tensorflow-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y8en96sYNud",
        "colab_type": "text"
      },
      "source": [
        "**Remove default cuda version, because tf 1.0.0 using 8.0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92qbcFoW1p3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "114c9b63-a31f-4f22-ee63-d8cb6e71517f"
      },
      "source": [
        "!apt-get remove cuda\n",
        "!apt-get autoremove cuda\n",
        "!apt-get purge cuda\n",
        "!apt-key del /var/cuda-repo-9-2-local/*.pub\n",
        "!rm -rf /var/cuda-repo-8-0-local-ga2/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-common-440\n",
            "0 upgraded, 0 newly installed, 1 to remove and 35 not upgraded.\n",
            "After this operation, 35.8 kB disk space will be freed.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Removing libnvidia-common-440 (440.100-0ubuntu0.18.04.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'cuda' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q0dVypZYcHA",
        "colab_type": "text"
      },
      "source": [
        "**Download cuda 8.0 and install**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxANQMjWFXGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b4cd27b-9fec-42b2-941a-3f575e20d30f"
      },
      "source": [
        "!sudo wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-8-0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-22 18:09:45--  https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.0.24\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.0.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?eUX1YkF9QUG7m4s3LbADOCVegdZ2s_Zi8Vz4fQ0BTHkj9z8bPleVZ-tdJkY3-iWej8Jtb1NtebTR7kgob4llDehjb--V-tzGDRbg0lENom9lUj2T7JrF4ge3Vrgas4CLT5Y9N0ROdcondpA97utZjmZNm938PrBHLcz49WTK-afvhCrvxjg2gx9mV8C2ipr11qJwxrji6YAr-0QFlMWin3QDdg [following]\n",
            "--2020-08-22 18:09:45--  https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?eUX1YkF9QUG7m4s3LbADOCVegdZ2s_Zi8Vz4fQ0BTHkj9z8bPleVZ-tdJkY3-iWej8Jtb1NtebTR7kgob4llDehjb--V-tzGDRbg0lENom9lUj2T7JrF4ge3Vrgas4CLT5Y9N0ROdcondpA97utZjmZNm938PrBHLcz49WTK-afvhCrvxjg2gx9mV8C2ipr11qJwxrji6YAr-0QFlMWin3QDdg\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1913589814 (1.8G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.78G  84.5MB/s    in 22s     \n",
            "\n",
            "2020-08-22 18:10:07 (83.5 MB/s) - ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb’ saved [1913589814/1913589814]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-8-0-local-ga2.\n",
            "(Reading database ... 144482 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Warning: The postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2\n",
            "Warning: seems to use apt-key (provided by apt) without depending on gnupg or gnupg2.\n",
            "Warning: This will BREAK in the future and should be fixed by the package maintainer(s).\n",
            "Note: Check first if apt-key functionality is needed at all - it probably isn't!\n",
            "Warning: apt-key should not be used in scripts (called from postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2)\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-8-0-local-ga2  InRelease\n",
            "Ign:1 file:/var/cuda-repo-8-0-local-ga2  InRelease\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  Release.gpg [819 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 file:/var/cuda-repo-8-0-local-ga2  Packages [22.7 kB]\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Ign:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,834 B]\n",
            "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,857 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [117 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,043 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,341 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [101 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [890 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,417 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.4 kB]\n",
            "Get:28 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [896 kB]\n",
            "Fetched 8,227 kB in 3s (2,394 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-8-0 cuda-command-line-tools-8-0 cuda-core-8-0 cuda-cublas-8-0\n",
            "  cuda-cublas-dev-8-0 cuda-cudart-8-0 cuda-cudart-dev-8-0 cuda-cufft-8-0\n",
            "  cuda-cufft-dev-8-0 cuda-curand-8-0 cuda-curand-dev-8-0 cuda-cusolver-8-0\n",
            "  cuda-cusolver-dev-8-0 cuda-cusparse-8-0 cuda-cusparse-dev-8-0\n",
            "  cuda-demo-suite-8-0 cuda-documentation-8-0 cuda-driver-dev-8-0\n",
            "  cuda-license-8-0 cuda-misc-headers-8-0 cuda-npp-8-0 cuda-npp-dev-8-0\n",
            "  cuda-nvgraph-8-0 cuda-nvgraph-dev-8-0 cuda-nvml-dev-8-0 cuda-nvrtc-8-0\n",
            "  cuda-nvrtc-dev-8-0 cuda-runtime-8-0 cuda-samples-8-0 cuda-toolkit-8-0\n",
            "  cuda-visual-tools-8-0\n",
            "0 upgraded, 31 newly installed, 0 to remove and 60 not upgraded.\n",
            "Need to get 0 B/1,312 MB of archives.\n",
            "After this operation, 2,079 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-8-0-local-ga2  cuda-license-8-0 8.0.61-1 [27.6 kB]\n",
            "Get:2 file:/var/cuda-repo-8-0-local-ga2  cuda-misc-headers-8-0 8.0.61-1 [1,077 kB]\n",
            "Get:3 file:/var/cuda-repo-8-0-local-ga2  cuda-core-8-0 8.0.61-1 [20.0 MB]\n",
            "Get:4 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-8-0 8.0.61-1 [135 kB]\n",
            "Get:5 file:/var/cuda-repo-8-0-local-ga2  cuda-driver-dev-8-0 8.0.61-1 [14.1 kB]\n",
            "Get:6 file:/var/cuda-repo-8-0-local-ga2  cuda-cudart-dev-8-0 8.0.61-1 [1,071 kB]\n",
            "Get:7 file:/var/cuda-repo-8-0-local-ga2  cuda-command-line-tools-8-0 8.0.61-1 [26.1 MB]\n",
            "Get:8 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-8-0 8.0.61-1 [9,585 kB]\n",
            "Get:9 file:/var/cuda-repo-8-0-local-ga2  cuda-nvrtc-dev-8-0 8.0.61-1 [10.8 kB]\n",
            "Get:10 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-8-0 8.0.61-1 [29.3 MB]\n",
            "Get:11 file:/var/cuda-repo-8-0-local-ga2  cuda-cusolver-dev-8-0 8.0.61-1 [6,816 kB]\n",
            "Get:12 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-8-0 8.0.61-1 [27.2 MB]\n",
            "Get:13 file:/var/cuda-repo-8-0-local-ga2  cuda-cublas-dev-8-0 8.0.61-1 [57.4 MB]\n",
            "Get:14 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-8-0 8.0.61-1 [117 MB]\n",
            "Get:15 file:/var/cuda-repo-8-0-local-ga2  cuda-cufft-dev-8-0 8.0.61-1 [94.8 MB]\n",
            "Get:16 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-8-0 8.0.61-1 [43.7 MB]\n",
            "Get:17 file:/var/cuda-repo-8-0-local-ga2  cuda-curand-dev-8-0 8.0.61-1 [67.7 MB]\n",
            "Get:18 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-8-0 8.0.61-1 [28.8 MB]\n",
            "Get:19 file:/var/cuda-repo-8-0-local-ga2  cuda-cusparse-dev-8-0 8.0.61-1 [29.6 MB]\n",
            "Get:20 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-8-0 8.0.61-1 [157 MB]\n",
            "Get:21 file:/var/cuda-repo-8-0-local-ga2  cuda-npp-dev-8-0 8.0.61-1 [82.3 MB]\n",
            "Get:22 file:/var/cuda-repo-8-0-local-ga2  cuda-samples-8-0 8.0.61-1 [101 MB]\n",
            "Get:23 file:/var/cuda-repo-8-0-local-ga2  cuda-documentation-8-0 8.0.61-1 [113 MB]\n",
            "Get:24 file:/var/cuda-repo-8-0-local-ga2  cuda-nvml-dev-8-0 8.0.61-1 [48.4 kB]\n",
            "Get:25 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-8-0 8.0.61-1 [2,948 kB]\n",
            "Get:26 file:/var/cuda-repo-8-0-local-ga2  cuda-nvgraph-dev-8-0 8.0.61-1 [3,028 kB]\n",
            "Get:27 file:/var/cuda-repo-8-0-local-ga2  cuda-visual-tools-8-0 8.0.61-1 [286 MB]\n",
            "Get:28 file:/var/cuda-repo-8-0-local-ga2  cuda-toolkit-8-0 8.0.61-1 [2,892 B]\n",
            "Get:29 file:/var/cuda-repo-8-0-local-ga2  cuda-runtime-8-0 8.0.61-1 [2,574 B]\n",
            "Get:30 file:/var/cuda-repo-8-0-local-ga2  cuda-demo-suite-8-0 8.0.61-1 [4,988 kB]\n",
            "Get:31 file:/var/cuda-repo-8-0-local-ga2  cuda-8-0 8.0.61-1 [2,556 B]\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 31.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cuda-license-8-0.\n",
            "(Reading database ... 144576 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-license-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-8-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-core-8-0.\n",
            "Preparing to unpack .../02-cuda-core-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-core-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-8-0.\n",
            "Preparing to unpack .../03-cuda-cudart-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-8-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-8-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-8-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-8-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-8-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-8-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-8-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-8-0.\n",
            "Preparing to unpack .../11-cuda-cublas-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-8-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-8-0.\n",
            "Preparing to unpack .../13-cuda-cufft-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-8-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-8-0.\n",
            "Preparing to unpack .../15-cuda-curand-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-8-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-8-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-8-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-8-0.\n",
            "Preparing to unpack .../19-cuda-npp-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-8-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-samples-8-0.\n",
            "Preparing to unpack .../21-cuda-samples-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-samples-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-documentation-8-0.\n",
            "Preparing to unpack .../22-cuda-documentation-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-8-0.\n",
            "Preparing to unpack .../23-cuda-nvml-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-8-0.\n",
            "Preparing to unpack .../24-cuda-nvgraph-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-8-0.\n",
            "Preparing to unpack .../25-cuda-nvgraph-dev-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-8-0.\n",
            "Preparing to unpack .../26-cuda-visual-tools-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-8-0.\n",
            "Preparing to unpack .../27-cuda-toolkit-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-runtime-8-0.\n",
            "Preparing to unpack .../28-cuda-runtime-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-8-0.\n",
            "Preparing to unpack .../29-cuda-demo-suite-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Selecting previously unselected package cuda-8-0.\n",
            "Preparing to unpack .../30-cuda-8-0_8.0.61-1_amd64.deb ...\n",
            "Unpacking cuda-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-license-8-0 (8.0.61-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-8.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-nvgraph-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvgraph-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-driver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvml-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cufft-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-misc-headers-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-nvrtc-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusolver-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-core-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-curand-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-npp-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cudart-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cublas-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-runtime-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-cusparse-dev-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-command-line-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-demo-suite-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-samples-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-visual-tools-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-documentation-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-toolkit-8-0 (8.0.61-1) ...\n",
            "Setting up cuda-8-0 (8.0.61-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htUccf4eYhrK",
        "colab_type": "text"
      },
      "source": [
        "**Tensorflow-gpu install for TF 1.0.0 (Important tf-gpu version is 1.0.0)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdqNFQNnxsnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "1cad9fbc-efee-4216-8458-337f8d842397"
      },
      "source": [
        "!pip install tensorflow-gpu==1.0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/e2/7389e7a1c10eb209ddabe0b35cca2e522a3985a1df7e07904c76d1d5609c/tensorflow_gpu-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (95.3MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 34kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.0) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow-gpu==1.0) (49.2.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcqXdU_SYs3w",
        "colab_type": "text"
      },
      "source": [
        "**Check Cuda version**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjb5J7BIwN9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "33ff0a2d-d395-4ef4-d28b-56316f82f2fe"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2016 NVIDIA Corporation\n",
            "Built on Tue_Jan_10_13:22:03_CST_2017\n",
            "Cuda compilation tools, release 8.0, V8.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJYHtkZYx_S",
        "colab_type": "text"
      },
      "source": [
        "**Check GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yx38iPJ6IzJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "5afd2dba-072b-44f4-a18e-dc5657633879"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gpu:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKjABqfLsDQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TaWEM5jwnU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00f562d2-eb63-435c-e590-f68e96151f57"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a60gjZon_SJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bjMbMjz_d6G",
        "colab_type": "text"
      },
      "source": [
        "########## PART 1 - DATA PREPROCESSING ##########"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIYgjZ2-_aVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset\n",
        "lines = open('/content/gdrive/My Drive/EDSA/Zeus/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('/content/gdrive/My Drive/EDSA/Zeus/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrCxuDjcALyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJledSRIAP5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ1Hxnh7A5hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var = id2line.get('L443817')\n",
        "var"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Ux7arAATIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line.get(conversation[i]))\n",
        "        answers.append(id2line.get(conversation[i+1]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raTK9JK1EYhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_questions = []\n",
        "_answers = []\n",
        "for question in questions: \n",
        "    if question != None : \n",
        "        _questions.append(question)\n",
        "for answer in answers: \n",
        "    if answer != None : \n",
        "        _answers.append(answer)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymcT8auSDjX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHQCYznODn_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in _questions:\n",
        "    question\n",
        "    clean_questions.append(clean_text(question))\n",
        " \n",
        "# Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in _answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Xka5srE57x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filtering out the questions and answers that are too short or too long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYeCESkRE8Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        " \n",
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 15\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1\n",
        " \n",
        "# Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1\n",
        " \n",
        "# Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
        " \n",
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upYIKT0hFsRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "answers_into_int = []\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)\n",
        " \n",
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlVMm_MmF546",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## PART 2 - BUILDING THE SEQ2SEQ MODEL ##########\n",
        "# Creating placeholders for the inputs and the targets\n",
        "def model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
        "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "    return inputs, targets, lr, keep_prob\n",
        " \n",
        "# Preprocessing the targets\n",
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets\n",
        " \n",
        "# Creating the Encoder RNN\n",
        "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
        "                                                                    cell_bw = encoder_cell,\n",
        "                                                                    sequence_length = sequence_length,\n",
        "                                                                    inputs = rnn_inputs,\n",
        "                                                                    dtype = tf.float32)\n",
        "    return encoder_state\n",
        " \n",
        "# Decoding the training set\n",
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              name = \"attn_dec_train\")\n",
        "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                              training_decoder_function,\n",
        "                                                                                                              decoder_embedded_input,\n",
        "                                                                                                              sequence_length,\n",
        "                                                                                                              scope = decoding_scope)\n",
        "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "    return output_function(decoder_output_dropout)\n",
        " \n",
        "# Decoding the test/validation set\n",
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                              encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              decoder_embeddings_matrix,\n",
        "                                                                              sos_id,\n",
        "                                                                              eos_id,\n",
        "                                                                              maximum_length,\n",
        "                                                                              num_words,\n",
        "                                                                              name = \"attn_dec_inf\")\n",
        "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                                test_decoder_function,\n",
        "                                                                                                                scope = decoding_scope)\n",
        "    return test_predictions\n",
        " \n",
        "# Creating the Decoder RNN\n",
        "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
        "        biases = tf.zeros_initializer()\n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      None,\n",
        "                                                                      scope = decoding_scope,\n",
        "                                                                      weights_initializer = weights,\n",
        "                                                                      biases_initializer = biases)\n",
        "        training_predictions = decode_training_set(encoder_state,\n",
        "                                                   decoder_cell,\n",
        "                                                   decoder_embedded_input,\n",
        "                                                   sequence_length,\n",
        "                                                   decoding_scope,\n",
        "                                                   output_function,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        test_predictions = decode_test_set(encoder_state,\n",
        "                                           decoder_cell,\n",
        "                                           decoder_embeddings_matrix,\n",
        "                                           word2int['<SOS>'],\n",
        "                                           word2int['<EOS>'],\n",
        "                                           sequence_length - 1,\n",
        "                                           num_words,\n",
        "                                           decoding_scope,\n",
        "                                           output_function,\n",
        "                                           keep_prob,\n",
        "                                           batch_size)\n",
        "    return training_predictions, test_predictions\n",
        " \n",
        "# Building the seq2seq model\n",
        "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
        "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                              answers_num_words + 1,\n",
        "                                                              encoder_embedding_size,\n",
        "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
        "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
        "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
        "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
        "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
        "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
        "                                                         decoder_embeddings_matrix,\n",
        "                                                         encoder_state,\n",
        "                                                         questions_num_words,\n",
        "                                                         sequence_length,\n",
        "                                                         rnn_size,\n",
        "                                                         num_layers,\n",
        "                                                         questionswords2int,\n",
        "                                                         keep_prob,\n",
        "                                                         batch_size)\n",
        "    return training_predictions, test_predictions"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18vDgNMsGJUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## PART 3 - TRAINING THE SEQ2SEQ MODEL ##########\n",
        "# Setting the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.00001\n",
        "keep_probability = 0.5\n",
        " \n",
        "# Defining a session\n",
        "tf.reset_default_graph()\n",
        "session = tf.InteractiveSession()\n",
        " \n",
        "# Loading the model inputs\n",
        "inputs, targets, lr, keep_prob = model_inputs()\n",
        " \n",
        "# Setting the sequence length\n",
        "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
        " \n",
        "# Getting the shape of the inputs tensor\n",
        "input_shape = tf.shape(inputs)\n",
        " \n",
        "# Getting the training and test predictions\n",
        "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                       targets,\n",
        "                                                       keep_prob,\n",
        "                                                       batch_size,\n",
        "                                                       sequence_length,\n",
        "                                                       len(answerswords2int),\n",
        "                                                       len(questionswords2int),\n",
        "                                                       encoding_embedding_size,\n",
        "                                                       decoding_embedding_size,\n",
        "                                                       rnn_size,\n",
        "                                                       num_layers,\n",
        "                                                       questionswords2int)\n",
        " \n",
        "# Setting up the Loss Error, the Optimizer and Gradient Clipping\n",
        "with tf.name_scope(\"optimization\"):\n",
        "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
        "                                                  targets,\n",
        "                                                  tf.ones([input_shape[0], sequence_length]))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    gradients = optimizer.compute_gradients(loss_error)\n",
        "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
        "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIUpFvzsGPtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding the sequences with the <PAD> token\n",
        "def apply_padding(batch_of_sequences, word2int):\n",
        "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
        " \n",
        "# Splitting the data into batches of questions and answers\n",
        "def split_into_batches(questions, answers, batch_size):\n",
        "    for batch_index in range(0, len(questions) // batch_size):\n",
        "        start_index = batch_index * batch_size\n",
        "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
        "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
        "        yield padded_questions_in_batch, padded_answers_in_batch"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDPEEBeJGnuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "baafd232-e187-4bd0-89ab-a1d7ac92d8de"
      },
      "source": [
        "# Splitting the questions and answers into training and validation sets\n",
        "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
        "training_questions = sorted_clean_questions[training_validation_split:]\n",
        "training_answers = sorted_clean_answers[training_validation_split:]\n",
        "validation_questions = sorted_clean_questions[:training_validation_split]\n",
        "validation_answers = sorted_clean_answers[:training_validation_split]\n",
        " \n",
        "# Training\n",
        "batch_index_check_training_loss = 100\n",
        "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = []\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 5\n",
        "checkpoint = \"chatbot_weights.ckpt\"\n",
        "session.run(tf.global_variables_initializer())\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
        "        starting_time = time.time()\n",
        "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
        "                                                                                               targets: padded_answers_in_batch,\n",
        "                                                                                               lr: learning_rate,\n",
        "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                                               keep_prob: keep_probability})\n",
        "        total_training_loss_error += batch_training_loss_error\n",
        "        ending_time = time.time()\n",
        "        batch_time = ending_time - starting_time\n",
        "        if batch_index % batch_index_check_training_loss == 0:\n",
        "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_index,\n",
        "                                                                                                                                       len(training_questions) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "            total_training_loss_error = 0\n",
        "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
        "            total_validation_loss_error = 0\n",
        "            starting_time = time.time()\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
        "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
        "                                                                       targets: padded_answers_in_batch,\n",
        "                                                                       lr: learning_rate,\n",
        "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                       keep_prob: 1})\n",
        "                total_validation_loss_error += batch_validation_loss_error\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
        "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
        "            learning_rate *= learning_rate_decay\n",
        "            if learning_rate < min_learning_rate:\n",
        "                learning_rate = min_learning_rate\n",
        "            list_validation_loss_error.append(average_validation_loss_error)\n",
        "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "                print('I speak better now!!')\n",
        "                early_stopping_check = 0\n",
        "                saver = tf.train.Saver()\n",
        "                saver.save(session, checkpoint)\n",
        "            else:\n",
        "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "                early_stopping_check += 1\n",
        "                if early_stopping_check == early_stopping_stop:\n",
        "                    break\n",
        "    if early_stopping_check == early_stopping_stop:\n",
        "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "        break\n",
        "print(\"Game Over\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   1/100, Batch:    0/131, Training Loss Error:  0.065, Training Time on 100 Batches: 224 seconds\n",
            "Validation Loss Error:  1.941, Batch Validation Time: 1 seconds\n",
            "I speak better now!!\n",
            "Epoch:   1/100, Batch:  100/131, Training Loss Error:  2.571, Training Time on 100 Batches: 22 seconds\n",
            "Validation Loss Error:  1.842, Batch Validation Time: 1 seconds\n",
            "I speak better now!!\n",
            "Epoch:   2/100, Batch:    0/131, Training Loss Error:  0.620, Training Time on 100 Batches: 20 seconds\n",
            "Validation Loss Error:  1.825, Batch Validation Time: 1 seconds\n",
            "I speak better now!!\n",
            "Epoch:   2/100, Batch:  100/131, Training Loss Error:  1.926, Training Time on 100 Batches: 22 seconds\n",
            "Validation Loss Error:  1.817, Batch Validation Time: 1 seconds\n",
            "I speak better now!!\n",
            "Epoch:   3/100, Batch:    0/131, Training Loss Error:  0.609, Training Time on 100 Batches: 19 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-13195b490ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                                                \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                                                \u001b[0msequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded_answers_in_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                                                                keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtotal_training_loss_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_training_loss_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mending_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0y3poRaagHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df26771d-9e59-4569-cb59-2fb0cf53221a"
      },
      "source": [
        "########## PART 4 - TESTING THE SEQ2SEQ MODEL ##########\n",
        "# Loading the weights and Running the session\n",
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(session, checkpoint)\n",
        " \n",
        "# Converting the questions from strings to lists of encoding integers\n",
        "def convert_string2int(question, word2int):\n",
        "    question = clean_text(question)\n",
        "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DataLossError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: Checksum does not match: stored 2887852214 vs. calculated on the restored bytes 384542867\n\t [[Node: save_2/RestoreV2_24 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_2/Const_0, save_2/RestoreV2_24/tensor_names, save_2/RestoreV2_24/shape_and_slices)]]\n\t [[Node: save_2/RestoreV2_20/_61 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_252_save_2/RestoreV2_20\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3bbe910d4708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Converting the questions from strings to lists of encoding integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: Checksum does not match: stored 2887852214 vs. calculated on the restored bytes 384542867\n\t [[Node: save_2/RestoreV2_24 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_2/Const_0, save_2/RestoreV2_24/tensor_names, save_2/RestoreV2_24/shape_and_slices)]]\n\t [[Node: save_2/RestoreV2_20/_61 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_252_save_2/RestoreV2_20\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'save_2/RestoreV2_24', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 462, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 492, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 444, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-25-3bbe910d4708>\", line 6, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): Checksum does not match: stored 2887852214 vs. calculated on the restored bytes 384542867\n\t [[Node: save_2/RestoreV2_24 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_2/Const_0, save_2/RestoreV2_24/tensor_names, save_2/RestoreV2_24/shape_and_slices)]]\n\t [[Node: save_2/RestoreV2_20/_61 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_252_save_2/RestoreV2_20\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iogX8LyPzVTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up the chat\n",
        "while(True):\n",
        "    question = input(\"You: \")\n",
        "    if question == 'Goodbye':\n",
        "        break\n",
        "    question = convert_string2int(question, questionswords2int)\n",
        "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
        "    fake_batch = np.zeros((batch_size, 25))\n",
        "    fake_batch[0] = question\n",
        "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
        "    answer = ''\n",
        "    for i in np.argmax(predicted_answer, 1):\n",
        "        if answersints2word[i] == 'i':\n",
        "            token = ' I'\n",
        "        elif answersints2word[i] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif answersints2word[i] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + answersints2word[i]\n",
        "        answer += token\n",
        "        if token == '.':\n",
        "            break\n",
        "    print('ChatBot: ' + answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6xOlgdn-5I8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}